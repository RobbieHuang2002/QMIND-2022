{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intrinsic Image Popularity Assessment\nThis notebook is an implementation of the model described by Ding et al. in the paper \"Intrinsic Image Popularity Assessment\" (https://arxiv.org/pdf/1907.01985.pdf).","metadata":{}},{"cell_type":"markdown","source":"### Import libraries\n- Torch: Build and train the dataset and model\n- Torchvision: Transform images and get the pretrained ResNet-50 model\n- os: Get image paths\n- PIL: Load images\n- tqdm: Fancy progress bar\n- matplotlib: Display images","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import transforms\nfrom torchvision.models import resnet50\nfrom torchvision.utils import make_grid\n\nimport os\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:47.979377Z","iopub.execute_input":"2022-11-20T00:53:47.980033Z","iopub.status.idle":"2022-11-20T00:53:49.881835Z","shell.execute_reply.started":"2022-11-20T00:53:47.979940Z","shell.execute_reply":"2022-11-20T00:53:49.880899Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Parameters\nParameters of the model as described in the paper.\n\n- `lr_pretrained`: Learning rate of ResNet-50\n- `lr_last`: Learning rate of the last linear layer\n- `opt_weight_decay`: L2 penalty multiplier for Adam\n- `lr_decay`: Learning rate decay\n- `batch_size`: Batch size\n- `criterion`: Loss function (binary cross-entropy)\n- `n_epochs`: Number of epochs\n- `img_rescale_dim`: length of shortest side of image after rescaling\n- `img_dim`: length and width of image after cropping\n\nAdditionally we specify the number of steps before showing results during training (`display_step`) and the device.","metadata":{}},{"cell_type":"code","source":"lr_pretrained = 1e-5\nlr_last = 1e-4\nopt_weight_decay = 1e-4\nlr_decay = 0.95\nbatch_size = 64\ncriterion = nn.BCEWithLogitsLoss()\nn_epochs = 14\n\nimg_rescale_dim = 256\nimg_dim = 224\n\ndisplay_step = 10\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:49.883795Z","iopub.execute_input":"2022-11-20T00:53:49.884329Z","iopub.status.idle":"2022-11-20T00:53:49.952254Z","shell.execute_reply.started":"2022-11-20T00:53:49.884292Z","shell.execute_reply":"2022-11-20T00:53:49.951224Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Helper Function: show_images\nCreate a helper function to display images. It first unnormalizes the images, and then displays `n_images` number of images. Change `n_row` to adjust the dimensions of the image grid.","metadata":{}},{"cell_type":"code","source":"inv_normalize = transforms.Normalize(\n    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], \n    std=[1/0.229, 1/0.224, 1/0.225]\n)\n\ndef show_images(images, normalized=True, size=(3, img_dim, img_dim), n_images=4, n_row=2):\n    \"\"\"\n    Helper function for displaying images.\n    images - The image tensor.\n    normalized - If the images are normalized as specified for the model\n    size - The size of each image (channels, height, width).\n    n_images - The number of images to be displayed.\n    n_row - The number of rows in the image grid.\n    \"\"\"\n    \n    # Inverse normalize\n    # Formula: mean = -mean/std, std = 1/std\n    if normalized:\n        images = inv_normalize(images)\n    \n    # Display images\n    image_unflat = images.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:n_images], nrow=n_row)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:49.953637Z","iopub.execute_input":"2022-11-20T00:53:49.955121Z","iopub.status.idle":"2022-11-20T00:53:49.966091Z","shell.execute_reply.started":"2022-11-20T00:53:49.955084Z","shell.execute_reply":"2022-11-20T00:53:49.965109Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Create dataset\nCreate training, validation, and test sets. \n\nDefine a paired dataset, where each item is a tuple (high popularity image, low popularity image). As specified in the paper, we perform the following preprocessing steps:\n1. Convert the image to RGB format\n2. Normalize the image to be between the range of [0, 1]\n3. Resize the image so that the shortest side has length 256\n4. Crop the image randomly to have dimensions 224 x 224\n5. Normalize the image to have a mean of [0.485, 0.456, 0.406] and a standard deviation of [0.229, 0.224, 0.225].","metadata":{}},{"cell_type":"code","source":"class PairedDataset(Dataset):\n    def __init__(self, im_1_paths, im_2_paths, transform):\n        self.im_1_paths = im_1_paths\n        self.im_2_paths = im_2_paths\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = Image.open(self.im_1_paths[index]).convert('RGB')\n        y = Image.open(self.im_2_paths[index]).convert('RGB')\n        \n        x = self.transform(x)\n        y = self.transform(y)\n\n        return x, y\n\n    def __len__(self):\n        return len(self.im_1_paths)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:49.969131Z","iopub.execute_input":"2022-11-20T00:53:49.969888Z","iopub.status.idle":"2022-11-20T00:53:49.976846Z","shell.execute_reply.started":"2022-11-20T00:53:49.969836Z","shell.execute_reply":"2022-11-20T00:53:49.976146Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Get image paths\nfile_list_high = []\nfile_list_low = []\n\nfor i in os.listdir('../input/pdip-instagram/High_popularity'):\n    file_list_high.append(f'../input/pdip-instagram/High_popularity/{i}')\n    file_list_low.append(f'../input/pdip-instagram/Low_popularity/{i}')\n    \nfile_list_high.sort()\nfile_list_low.sort()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:49.978271Z","iopub.execute_input":"2022-11-20T00:53:49.978897Z","iopub.status.idle":"2022-11-20T00:53:50.419587Z","shell.execute_reply.started":"2022-11-20T00:53:49.978862Z","shell.execute_reply":"2022-11-20T00:53:50.418598Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Preprocess images\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(img_rescale_dim),\n    transforms.RandomCrop(img_dim),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:50.421127Z","iopub.execute_input":"2022-11-20T00:53:50.421822Z","iopub.status.idle":"2022-11-20T00:53:50.427827Z","shell.execute_reply.started":"2022-11-20T00:53:50.421787Z","shell.execute_reply":"2022-11-20T00:53:50.426814Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Split data into train, validation, and test sets\nval_split = len(file_list_high) * 8 // 10\ntest_split = len(file_list_high) * 9 // 10\ntrain_ds = PairedDataset(file_list_high[:val_split], file_list_low[:val_split], transform)\nval_ds = PairedDataset(file_list_high[val_split:test_split], file_list_low[val_split:test_split], transform)\ntest_ds = PairedDataset(file_list_high[test_split:], file_list_low[test_split:], transform)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:50.429875Z","iopub.execute_input":"2022-11-20T00:53:50.430857Z","iopub.status.idle":"2022-11-20T00:53:50.438921Z","shell.execute_reply.started":"2022-11-20T00:53:50.430810Z","shell.execute_reply":"2022-11-20T00:53:50.437973Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Create the model\nLoad the pretrained ResNet-50 model which is trained on the ImageNet-1k dataset. We fine-tune the base model by replacing the output layer with a fully-connected layer that consists of a single neuron.\n\nAs specified in the paper, we have different learning rates for the base model and the fully-connected layer. We also define a learning rate decay that is updated every epoch.","metadata":{}},{"cell_type":"code","source":"model = resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 1)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:50.440509Z","iopub.execute_input":"2022-11-20T00:53:50.441195Z","iopub.status.idle":"2022-11-20T00:53:55.440194Z","shell.execute_reply.started":"2022-11-20T00:53:50.441104Z","shell.execute_reply":"2022-11-20T00:53:55.438965Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41006706b7b4025a96a2bcec43493c2"}},"metadata":{}}]},{"cell_type":"code","source":"pretrained_params = []\nfor name, param in model.named_parameters():\n    if name != 'fc.weight' and name != 'fc.bias':\n        pretrained_params.append(param)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:55.441832Z","iopub.execute_input":"2022-11-20T00:53:55.443610Z","iopub.status.idle":"2022-11-20T00:53:55.458469Z","shell.execute_reply.started":"2022-11-20T00:53:55.443567Z","shell.execute_reply":"2022-11-20T00:53:55.456937Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"opt = torch.optim.Adam([\n        {'params': pretrained_params, 'lr': lr_pretrained}, \n        {'params': model.fc.parameters(), 'lr': lr_last}\n], weight_decay=opt_weight_decay)\nscheduler = StepLR(opt, 1, gamma=lr_decay, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:55.463426Z","iopub.execute_input":"2022-11-20T00:53:55.463827Z","iopub.status.idle":"2022-11-20T00:53:55.475825Z","shell.execute_reply.started":"2022-11-20T00:53:55.463794Z","shell.execute_reply":"2022-11-20T00:53:55.474281Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Adjusting learning rate of group 0 to 1.0000e-05.\nAdjusting learning rate of group 1 to 1.0000e-04.\n","output_type":"stream"}]},{"cell_type":"code","source":"pretrained = False\nif pretrained:\n    loaded_state = torch.load(\"../input/resnet50-checkpoints/resnet50_12.pth\", map_location=device)\n    model.load_state_dict(loaded_state['model'])\n    opt.load_state_dict(loaded_state['opt'])\n    scheduler.load_state_dict(loaded_state['scheduler'])\nelse:\n    nn.init.kaiming_normal_(model.fc.weight, mode='fan_in')","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:55.477559Z","iopub.execute_input":"2022-11-20T00:53:55.478006Z","iopub.status.idle":"2022-11-20T00:53:58.705244Z","shell.execute_reply.started":"2022-11-20T00:53:55.477963Z","shell.execute_reply":"2022-11-20T00:53:58.704286Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Train model\nTrain the model on the PDIP dataset by using a Siamese architecture. First we pass the high and low popularity images separately through the model and then store the output value as the image's popularity score. Then we subtract the low popularity score from the high popularity score to get the model's prediction as to which image is more popular. \n\nThrough the binary cross entropy with logits loss, we calculate the sigmoid of the difference. If the model predicts that the high popularity image is more popular, it will output a value close to 1. On the other hand, if the model predicts that the low popularity image is more popular, it will output a value close to 0. We then take this value and compare it to the ground truth value of 1, where a value closer to 1 corresponds to a lower loss.","metadata":{}},{"cell_type":"code","source":"def train(save_model=True):\n    dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    cur_step = 0\n    total_loss = 0\n    \n    for epoch in range(n_epochs):\n        for high, low in tqdm(dataloader):\n            high = high.to(device)\n            low = low.to(device)\n            \n            high_pred = model(high)\n            low_pred = model(low)\n            print(high_pred, low_pred)\n            \n            loss = criterion(high_pred - low_pred, torch.ones_like(high_pred))\n            \n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            \n            total_loss += loss.item()\n            \n            if cur_step % display_step == 0 and display_step > 0:\n                print(f\"Epoch: {epoch} \\t Step: {cur_step} \\t Loss: {total_loss / display_step}\")\n                total_loss = 0\n            cur_step += 1\n            \n        scheduler.step()\n        \n        if save_model:\n            torch.save({\n                \"model\": model.state_dict(),\n                \"opt\": opt.state_dict(),\n                \"scheduler\": scheduler.state_dict()\n            }, f\"resnet50_{epoch}.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:58.715191Z","iopub.execute_input":"2022-11-20T00:53:58.715703Z","iopub.status.idle":"2022-11-20T00:53:58.725944Z","shell.execute_reply.started":"2022-11-20T00:53:58.715661Z","shell.execute_reply":"2022-11-20T00:53:58.724996Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:53:58.727495Z","iopub.execute_input":"2022-11-20T00:53:58.727886Z","iopub.status.idle":"2022-11-20T00:53:58.737295Z","shell.execute_reply.started":"2022-11-20T00:53:58.727853Z","shell.execute_reply":"2022-11-20T00:53:58.736275Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Validation\nCheck the model with the validation set. If the model did not overfit the training data, it should output a loss close to the loss seen during training.","metadata":{}},{"cell_type":"code","source":"def validate():\n    dataloader = DataLoader(val_ds, batch_size=batch_size)\n    total_loss = 0\n    \n    for high, low in tqdm(dataloader):\n        high = high.to(device)\n        low = low.to(device)\n        \n        with torch.no_grad():\n            high_pred = model(high)\n            low_pred = model(low)\n\n            loss = criterion(high_pred - low_pred, torch.ones_like(high_pred))\n            total_loss += loss.item()\n            \n    print(\"Loss:\", total_loss / len(dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:56:46.734679Z","iopub.execute_input":"2022-11-20T00:56:46.735055Z","iopub.status.idle":"2022-11-20T00:56:46.743140Z","shell.execute_reply.started":"2022-11-20T00:56:46.735025Z","shell.execute_reply":"2022-11-20T00:56:46.742242Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"validate()","metadata":{"execution":{"iopub.status.busy":"2022-12-01T22:26:00.775780Z","iopub.execute_input":"2022-12-01T22:26:00.776827Z","iopub.status.idle":"2022-12-01T22:26:00.797795Z","shell.execute_reply.started":"2022-12-01T22:26:00.776720Z","shell.execute_reply":"2022-12-01T22:26:00.796976Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def predict(img_path, transform):\n    img = Image.open(img_path).convert('RGB')\n    img = transform(img).to(device)\n    img = torch.unsqueeze(img, 0)\n    \n    with torch.no_grad():\n        img_pred = model(img)\n        \n    return img_pred","metadata":{"execution":{"iopub.status.busy":"2022-11-20T00:54:11.151061Z","iopub.execute_input":"2022-11-20T00:54:11.151673Z","iopub.status.idle":"2022-11-20T00:54:11.159360Z","shell.execute_reply.started":"2022-11-20T00:54:11.151636Z","shell.execute_reply":"2022-11-20T00:54:11.158351Z"},"trusted":true},"execution_count":17,"outputs":[]}]}